{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uuCR4lU7hcs"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import math\n",
        "import random\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# try to import torch and sklearn; if not available, instruct user to install via generated requirements\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    from torch.utils.data import Dataset, DataLoader\n",
        "except Exception:\n",
        "    print(\"PyTorch not available. Please install packages listed in the generated requirements.txt (run run_all.sh).\")\n",
        "    # allow the script to still write helper files even if torch missing\n",
        "    torch = None\n",
        "    nn = None\n",
        "    Dataset = object\n",
        "    DataLoader = None\n",
        "\n",
        "try:\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "except Exception:\n",
        "    print(\"scikit-learn not available. Please install packages listed in the generated requirements.txt (run run_all.sh).\")\n",
        "    StandardScaler = None\n",
        "    mean_squared_error = None\n",
        "    mean_absolute_error = None\n",
        "\n",
        "# Local brief images (from your upload)\n",
        "BRIEF_IMAGE_1 = \"/mnt/data/Arun1.png\"\n",
        "BRIEF_IMAGE_2 = \"/mnt/data/Arun2.png\"\n",
        "\n",
        "# Output directories\n",
        "DATA_DIR = \"data\"\n",
        "OUT_DIR = \"results\"\n",
        "MODEL_DIR = os.path.join(OUT_DIR, \"models\")\n",
        "FIG_DIR = os.path.join(OUT_DIR, \"figs\")\n",
        "ATTN_DIR = os.path.join(FIG_DIR, \"attention\")\n",
        "EXPERIMENTS_DIR = \"experiments\"\n",
        "TESTS_DIR = \"tests\"\n",
        "\n",
        "for d in [DATA_DIR, OUT_DIR, MODEL_DIR, FIG_DIR, ATTN_DIR, EXPERIMENTS_DIR, TESTS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch is not None:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    denom = np.where(np.abs(y_true) < 1e-8, 1e-8, np.abs(y_true))\n",
        "    return np.mean(np.abs((y_true - y_pred) / denom)) * 100\n",
        "\n",
        "# ---------------------------\n",
        "# Synthetic dataset\n",
        "# ---------------------------\n",
        "def generate_synthetic(length=15000, freq='H', seed=0, out_csv=None):\n",
        "    \"\"\"Generate a univariate time series with trend, seasonality, volatility bursts and changepoints.\"\"\"\n",
        "    seed_everything(seed)\n",
        "    start = datetime(2015, 1, 1)\n",
        "    if freq == 'H':\n",
        "        dates = [start + timedelta(hours=i) for i in range(length)]\n",
        "    elif freq == 'D':\n",
        "        dates = [start + timedelta(days=i) for i in range(length)]\n",
        "    else:\n",
        "        dates = [start + timedelta(minutes=i) for i in range(length)]\n",
        "    t = np.arange(length)\n",
        "    trend = 0.0002 * (t ** 1.05)\n",
        "    daily = 3.0 * np.sin(2 * np.pi * (t % 24) / 24)\n",
        "    weekly = 1.5 * np.sin(2 * np.pi * (t % (24*7)) / (24*7))\n",
        "    # changepoints\n",
        "    cps = [int(length * 0.2), int(length * 0.5), int(length * 0.75)]\n",
        "    cp_effect = np.zeros_like(t, dtype=float)\n",
        "    for i, cp in enumerate(cps):\n",
        "        cp_effect += (t > cp) * (0.5 * (i + 1))\n",
        "    # volatility bursts\n",
        "    noise = np.random.normal(0, 0.6, size=length)\n",
        "    bursts = [int(length * 0.35), int(length * 0.6)]\n",
        "    for c in bursts:\n",
        "        width = int(length * 0.01) if length>10000 else 200\n",
        "        window = np.exp(-((t - c) ** 2) / (2 * (width ** 2)))\n",
        "        noise += window * np.random.normal(0, 8.0, size=length)\n",
        "    y = 10 + trend + daily + weekly + cp_effect + noise\n",
        "    df = pd.DataFrame({'ds': dates, 'y': y})\n",
        "    if out_csv:\n",
        "        df.to_csv(out_csv, index=False)\n",
        "    return df\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset and DataLoader\n",
        "# ---------------------------\n",
        "class TimeSeriesWindowDataset(Dataset):\n",
        "    def __init__(self, series: np.ndarray, input_len: int, horizon: int):\n",
        "        # series should be 1D numpy\n",
        "        if series.ndim == 1:\n",
        "            series = series.reshape(-1, 1)\n",
        "        self.series = series.astype(np.float32)\n",
        "        self.input_len = input_len\n",
        "        self.horizon = horizon\n",
        "        self.T = series.shape[0]\n",
        "        self.n_features = series.shape[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, self.T - self.input_len - self.horizon + 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx\n",
        "        x = self.series[start: start + self.input_len]\n",
        "        y = self.series[start + self.input_len: start + self.input_len + self.horizon]\n",
        "        return x, y\n",
        "\n",
        "def collate_windows(batch):\n",
        "    xs = [torch.tensor(b[0]) for b in batch]\n",
        "    ys = [torch.tensor(b[1]) for b in batch]\n",
        "    x = torch.stack(xs)  # (B, input_len, feat)\n",
        "    y = torch.stack(ys)  # (B, horizon, feat)\n",
        "    return x.permute(0, 2, 1), y.permute(0, 2, 1)  # (B, C, L), (B, C, H)\n",
        "\n",
        "# ---------------------------\n",
        "# Robust attention hook registration\n",
        "# ---------------------------\n",
        "def register_transformer_attention_hooks(transformer_encoder, attn_container):\n",
        "    \"\"\"\n",
        "    Register forward hooks on TransformerEncoder layers' MultiheadAttention modules.\n",
        "    Collected tensors appended to attn_container (list).\n",
        "    \"\"\"\n",
        "    # transformer_encoder.layers is a ModuleList of TransformerEncoderLayer\n",
        "    for i, layer in enumerate(getattr(transformer_encoder, \"layers\", [])):\n",
        "        mha = getattr(layer, \"self_attn\", None)\n",
        "        if mha is None:\n",
        "            continue\n",
        "        def make_hook(idx):\n",
        "            def hook(module, inp, out):\n",
        "                # out often is attn_output (Tensor) or tuple where second element is attn_weights.\n",
        "                attn_weights = None\n",
        "                if isinstance(out, tuple) and len(out) >= 2:\n",
        "                    attn_weights = out[1]\n",
        "                else:\n",
        "                    # try module.attn_output_weights or module.attn_weights depending on implementation\n",
        "                    attn_weights = getattr(module, \"attn_output_weights\", None) or getattr(module, \"attn_weights\", None)\n",
        "                if attn_weights is not None:\n",
        "                    try:\n",
        "                        attn_container.append(attn_weights.detach().cpu())\n",
        "                    except Exception:\n",
        "                        pass\n",
        "            return hook\n",
        "        try:\n",
        "            mha.register_forward_hook(make_hook(i))\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: failed to register hook on layer {i}: {e}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Models\n",
        "# ---------------------------\n",
        "if torch is not None:\n",
        "    class SimplePositionalEncoding(nn.Module):\n",
        "        def __init__(self, d_model, max_len=2000):\n",
        "            super().__init__()\n",
        "            pe = torch.zeros(max_len, d_model)\n",
        "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "            pe[:, 0::2] = torch.sin(position * div_term)\n",
        "            pe[:, 1::2] = torch.cos(position * div_term)\n",
        "            pe = pe.unsqueeze(0).transpose(1, 2)  # (1, d_model, max_len)\n",
        "            self.register_buffer('pe', pe)\n",
        "\n",
        "        def forward(self, x):\n",
        "            # x: (B, d_model, L)\n",
        "            L = x.size(-1)\n",
        "            return x + self.pe[:, :, :L]\n",
        "\n",
        "    class TransformerForecaster(nn.Module):\n",
        "        def __init__(self, input_dim=1, d_model=64, n_heads=4, n_layers=3, d_ff=128,\n",
        "                     dropout=0.1, horizon=50, input_len=256, pos_max_len=2048):\n",
        "            super().__init__()\n",
        "            self.input_dim = input_dim\n",
        "            self.d_model = d_model\n",
        "            self.horizon = horizon\n",
        "            self.input_len = input_len\n",
        "\n",
        "            self.input_proj = nn.Conv1d(in_channels=input_dim, out_channels=d_model, kernel_size=1)\n",
        "            self.pos_enc = SimplePositionalEncoding(d_model, max_len=max(pos_max_len, input_len))\n",
        "\n",
        "            encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads,\n",
        "                                                       dim_feedforward=d_ff, dropout=dropout,\n",
        "                                                       activation='gelu', batch_first=True)\n",
        "            self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "            self.head = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool1d(1),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(d_model, d_model),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(d_model, horizon * input_dim)\n",
        "            )\n",
        "            # container for attention weights captured by hooks\n",
        "            self._attn_weights = []\n",
        "            # register hooks robustly\n",
        "            try:\n",
        "                register_transformer_attention_hooks(self.encoder, self._attn_weights)\n",
        "            except Exception as e:\n",
        "                print(\"Warning: registering attention hooks failed:\", e)\n",
        "\n",
        "        def forward(self, x, return_attn=False):\n",
        "            # x: (B, C, L)\n",
        "            B, C, L = x.shape\n",
        "            x = self.input_proj(x)  # (B, d_model, L)\n",
        "            x = self.pos_enc(x)\n",
        "            x_t = x.permute(0, 2, 1)  # (B, L, d_model)\n",
        "            # clear previously collected weights\n",
        "            self._attn_weights = []\n",
        "            enc = self.encoder(x_t)\n",
        "            enc_p = enc.permute(0, 2, 1)\n",
        "            out = self.head(enc_p)\n",
        "            out = out.view(B, self.input_dim, self.horizon)\n",
        "            if return_attn:\n",
        "                # return a shallow copy to avoid modification during further forward calls\n",
        "                return out, list(self._attn_weights)\n",
        "            else:\n",
        "                return out\n",
        "\n",
        "    class LSTMForecaster(nn.Module):\n",
        "        def __init__(self, input_dim=1, hidden=128, n_layers=2, horizon=50):\n",
        "            super().__init__()\n",
        "            self.input_dim = input_dim\n",
        "            self.horizon = horizon\n",
        "            self.rnn = nn.LSTM(input_dim, hidden, n_layers, batch_first=True, dropout=0.1)\n",
        "            self.head = nn.Sequential(nn.Linear(hidden, hidden), nn.GELU(), nn.Linear(hidden, horizon * input_dim))\n",
        "\n",
        "        def forward(self, x):\n",
        "            # x: (B, C, L) -> permute to (B, L, C)\n",
        "            x = x.permute(0, 2, 1)\n",
        "            out, _ = self.rnn(x)\n",
        "            last = out[:, -1, :]\n",
        "            preds = self.head(last).view(x.size(0), self.input_dim, self.horizon)\n",
        "            return preds\n",
        "\n",
        "else:\n",
        "    TransformerForecaster = None\n",
        "    LSTMForecaster = None\n",
        "\n",
        "# ---------------------------\n",
        "# Prophet baseline wrapper (optional)\n",
        "# ---------------------------\n",
        "def try_import_prophet():\n",
        "    try:\n",
        "        from prophet import Prophet\n",
        "        return Prophet\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "class ProphetWrapper:\n",
        "    \"\"\"\n",
        "    Lightweight Prophet wrapper. Uses pandas Series indexed by datetimes.\n",
        "    Note: Prophet fits can be slow when run per sliding window.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        Prophet = try_import_prophet()\n",
        "        if Prophet is None:\n",
        "            raise ImportError(\"prophet not installed. Install via requirements.txt (run_all.sh).\")\n",
        "        self.model = Prophet(**kwargs)\n",
        "\n",
        "    def fit(self, series: pd.Series):\n",
        "        df = pd.DataFrame({'ds': series.index, 'y': series.values})\n",
        "        self.model.fit(df)\n",
        "\n",
        "    def forecast_periods(self, periods, freq='H'):\n",
        "        future = self.model.make_future_dataframe(periods=periods, freq=freq)\n",
        "        fc = self.model.predict(future)\n",
        "        return fc[['ds', 'yhat']].tail(periods)['yhat'].values\n",
        "\n",
        "# ---------------------------\n",
        "# Training + evaluation helpers\n",
        "# ---------------------------\n",
        "def train_one_epoch(model, loader, opt, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        if isinstance(model, TransformerForecaster):\n",
        "            preds, _ = model(xb, return_attn=True)\n",
        "        else:\n",
        "            preds = model(xb)\n",
        "        loss = loss_fn(preds, yb)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        count += xb.size(0)\n",
        "    return total_loss / max(1, count)\n",
        "\n",
        "def validate(model, loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            if isinstance(model, TransformerForecaster):\n",
        "                preds, _ = model(xb, return_attn=True)\n",
        "            else:\n",
        "                preds = model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "            total_loss += loss.item() * xb.size(0)\n",
        "            count += xb.size(0)\n",
        "    return total_loss / max(1, count)\n",
        "\n",
        "def save_checkpoint(path, model, model_args):\n",
        "    tosave = {'state_dict': model.state_dict(), 'model_args': model_args}\n",
        "    torch.save(tosave, path)\n",
        "\n",
        "def evaluate_model_checkpoint(checkpoint_path, model_cls, scaler, test_series, input_len, horizon, device, batch_size=64):\n",
        "    ckpt = torch.load(checkpoint_path, map_location=device)\n",
        "    model = model_cls(**ckpt['model_args']).to(device)\n",
        "    model.load_state_dict(ckpt['state_dict'])\n",
        "    model.eval()\n",
        "    ds = test_series.copy()\n",
        "    ds_scaled = scaler.transform(ds.reshape(-1, 1)).flatten()\n",
        "    dataset = TimeSeriesWindowDataset(ds_scaled, input_len=input_len, horizon=horizon)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_windows)\n",
        "    preds = []\n",
        "    trues = []\n",
        "    attn_collected = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            if isinstance(model, TransformerForecaster):\n",
        "                py, attn = model(xb, return_attn=True)\n",
        "                if attn:\n",
        "                    attn_collected.extend(attn)\n",
        "            else:\n",
        "                py = model(xb)\n",
        "            py = py.cpu().numpy()\n",
        "            yb = yb.cpu().numpy()\n",
        "            preds.append(py)\n",
        "            trues.append(yb)\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    trues = np.concatenate(trues, axis=0)\n",
        "    B, C, H = preds.shape\n",
        "    preds_flat = preds.transpose(0, 2, 1).reshape(-1, C)\n",
        "    trues_flat = trues.transpose(0, 2, 1).reshape(-1, C)\n",
        "    preds_inv = scaler.inverse_transform(preds_flat).reshape(-1, H, C).transpose(0, 2, 1)\n",
        "    trues_inv = scaler.inverse_transform(trues_flat).reshape(-1, H, C).transpose(0, 2, 1)\n",
        "    return preds_inv, trues_inv, attn_collected\n",
        "\n",
        "# ---------------------------\n",
        "# High-level experiment runner\n",
        "# ---------------------------\n",
        "def run_experiment(args):\n",
        "    if torch is None:\n",
        "        raise RuntimeError(\"PyTorch is required to train models. Install dependencies via generated requirements.txt and run run_all.sh.\")\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu')\n",
        "    seed_everything(args.seed)\n",
        "\n",
        "    # 1) Data\n",
        "    if args.generate:\n",
        "        df = generate_synthetic(length=args.length, freq=args.freq, seed=args.seed, out_csv=os.path.join(DATA_DIR, f'synthetic_{args.length}.csv'))\n",
        "        csv_path = os.path.join(DATA_DIR, f'synthetic_{args.length}.csv')\n",
        "    else:\n",
        "        csv_path = args.data_path\n",
        "        df = pd.read_csv(csv_path)\n",
        "    series = df['y'].values.astype(float)\n",
        "\n",
        "    # quick plot first 2000 points\n",
        "    plt.figure(figsize=(12,3))\n",
        "    plt.plot(df['ds'].iloc[:2000], series[:2000])\n",
        "    plt.title('Sample series (first 2000 timesteps)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(FIG_DIR, 'sample_series.png'))\n",
        "    plt.close()\n",
        "\n",
        "    T = len(series)\n",
        "    train_end = int(T * 0.7)\n",
        "    val_end = int(T * 0.85)\n",
        "    train_series = series[:train_end]\n",
        "    val_series = series[train_end:val_end]\n",
        "    test_series = series[val_end:]\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    train_scaled = scaler.fit_transform(train_series.reshape(-1,1)).flatten()\n",
        "    val_scaled = scaler.transform(val_series.reshape(-1,1)).flatten()\n",
        "    test_scaled = scaler.transform(test_series.reshape(-1,1)).flatten()\n",
        "\n",
        "    # persist scaler\n",
        "    with open(os.path.join(OUT_DIR, 'scaler.json'), 'w') as f:\n",
        "        json.dump({'mean': float(scaler.mean_[0]), 'scale': float(scaler.scale_[0])}, f)\n",
        "\n",
        "    # datasets\n",
        "    input_len = args.input_len\n",
        "    horizon = args.horizon\n",
        "    train_ds = TimeSeriesWindowDataset(train_scaled, input_len, horizon)\n",
        "    val_ds = TimeSeriesWindowDataset(val_scaled, input_len, horizon)\n",
        "    test_ds = TimeSeriesWindowDataset(test_scaled, input_len, horizon)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, collate_fn=collate_windows)\n",
        "    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_windows)\n",
        "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_windows)\n",
        "\n",
        "    # model\n",
        "    if args.model == 'transformer':\n",
        "        model = TransformerForecaster(input_dim=1, d_model=args.d_model, n_heads=args.n_heads,\n",
        "                                      n_layers=args.n_layers, d_ff=args.d_ff, dropout=args.dropout,\n",
        "                                      horizon=horizon, input_len=input_len)\n",
        "        model_args = {'input_dim':1, 'd_model':args.d_model, 'n_heads':args.n_heads, 'n_layers':args.n_layers,\n",
        "                      'd_ff':args.d_ff, 'dropout':args.dropout, 'horizon':horizon, 'input_len':input_len}\n",
        "    else:\n",
        "        model = LSTMForecaster(input_dim=1, hidden=args.hidden, n_layers=args.n_layers, horizon=horizon)\n",
        "        model_args = {'input_dim':1, 'hidden':args.hidden, 'n_layers':args.n_layers, 'horizon':horizon}\n",
        "\n",
        "    model = model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    best_val = float('inf')\n",
        "    best_path = None\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        tr_loss = train_one_epoch(model, train_loader, opt, loss_fn, device)\n",
        "        val_loss = validate(model, val_loader, loss_fn, device)\n",
        "        history.append({'epoch': epoch, 'train_loss': tr_loss, 'val_loss': val_loss})\n",
        "        print(f\"Epoch {epoch} â€” train_loss {tr_loss:.6f}, val_loss {val_loss:.6f}\")\n",
        "\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            best_path = os.path.join(MODEL_DIR, f'best_{args.model}.pth')\n",
        "            save_checkpoint(best_path, model, model_args)\n",
        "            print(\"Saved best model to\", best_path)\n",
        "\n",
        "        # simple early stopping\n",
        "        if epoch > args.patience and all(h['val_loss'] >= history[-args.patience]['val_loss'] for h in history[-args.patience:]):\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "    pd.DataFrame(history).to_csv(os.path.join(OUT_DIR, f'train_history_{args.model}.csv'), index=False)\n",
        "\n",
        "    # Evaluate across multiple horizons\n",
        "    metrics_records = []\n",
        "    horizons_to_eval = [10, 50, 100]\n",
        "    for h in horizons_to_eval:\n",
        "        print(\"Evaluating horizon\", h)\n",
        "        if best_path is None:\n",
        "            print(\"No checkpoint found, skipping evaluation\")\n",
        "            break\n",
        "        preds_inv, trues_inv, attn = evaluate_model_checkpoint(best_path,\n",
        "                                                                TransformerForecaster if args.model=='transformer' else LSTMForecaster,\n",
        "                                                                scaler, test_series, input_len, h, device, batch_size=args.batch_size)\n",
        "        N, C, H = preds_inv.shape\n",
        "        # compute metrics for first and mean across horizon\n",
        "        for metric_h in [1, H]:\n",
        "            preds_slice = preds_inv[:, 0, metric_h-1]\n",
        "            trues_slice = trues_inv[:, 0, metric_h-1]\n",
        "            rmse = math.sqrt(mean_squared_error(trues_slice, preds_slice))\n",
        "            mae = mean_absolute_error(trues_slice, preds_slice)\n",
        "            mape_v = mape(trues_slice, preds_slice)\n",
        "            metrics_records.append({'horizon_eval': h, 'step': metric_h, 'rmse': rmse, 'mae': mae, 'mape': mape_v})\n",
        "        # attention visualizations\n",
        "        if args.model == 'transformer' and attn:\n",
        "            os.makedirs(ATTN_DIR, exist_ok=True)\n",
        "            for i, a in enumerate(attn[:6]):\n",
        "                try:\n",
        "                    arr = a.numpy()\n",
        "                    # handle shapes: (B*num_heads, L, L) or (B, L, L) or (B, num_heads, tgt, src)\n",
        "                    if arr.ndim == 4:\n",
        "                        # (B, num_heads, tgt, src) -> average over batch then plot per-head or averaged\n",
        "                        arrm = arr.mean(axis=0)  # (num_heads, tgt, src)\n",
        "                        # plot mean over heads\n",
        "                        arrm_mean = arrm.mean(axis=0)\n",
        "                    elif arr.ndim == 3:\n",
        "                        # (B, tgt, src) or (num_heads, tgt, src) depending on version; average properly\n",
        "                        arrm_mean = arr.mean(axis=0)\n",
        "                    else:\n",
        "                        arrm_mean = arr\n",
        "                    plt.figure(figsize=(6,4))\n",
        "                    plt.imshow(arrm_mean, aspect='auto')\n",
        "                    plt.colorbar()\n",
        "                    plt.title(f'Attention map sample {i}')\n",
        "                    fname = os.path.join(ATTN_DIR, f'attn_map_{i}.png')\n",
        "                    plt.savefig(fname)\n",
        "                    plt.close()\n",
        "                except Exception as e:\n",
        "                    print(\"Failed to plot attention\", e)\n",
        "\n",
        "    pd.DataFrame(metrics_records).to_csv(os.path.join(OUT_DIR, f'metrics_{args.model}.csv'), index=False)\n",
        "    print(\"Saved metrics to\", os.path.join(OUT_DIR, f'metrics_{args.model}.csv'))\n",
        "\n",
        "    print(\"Experiment completed. Brief images referenced at:\", BRIEF_IMAGE_1, BRIEF_IMAGE_2)\n",
        "\n",
        "# ---------------------------\n",
        "# Optuna HP-search (simple integrated function)\n",
        "# ---------------------------\n",
        "def run_hp_search(args):\n",
        "    try:\n",
        "        import optuna\n",
        "    except Exception:\n",
        "        raise ImportError(\"Optuna not installed. Generate requirements and install (run run_all.sh).\")\n",
        "\n",
        "    def objective(trial):\n",
        "        # load data\n",
        "        df = pd.read_csv(args.data_path)\n",
        "        series = df['y'].values.astype(float)\n",
        "        T = len(series)\n",
        "        train_end = int(T * 0.7)\n",
        "        val_end = int(T * 0.85)\n",
        "        train_series = series[:train_end]\n",
        "        val_series = series[train_end:val_end]\n",
        "        scaler = StandardScaler().fit(train_series.reshape(-1,1))\n",
        "        train_scaled = scaler.transform(train_series.reshape(-1,1)).flatten()\n",
        "        val_scaled = scaler.transform(val_series.reshape(-1,1)).flatten()\n",
        "        # sample hyperparams\n",
        "        d_model = trial.suggest_categorical('d_model', [64, 128, 256])\n",
        "        n_heads = trial.suggest_categorical('n_heads', [2, 4, 8])\n",
        "        n_layers = trial.suggest_int('n_layers', 1, 4)\n",
        "        lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
        "        d_ff = d_model * 2\n",
        "        model = TransformerForecaster(input_dim=1, d_model=d_model, n_heads=n_heads, n_layers=n_layers, d_ff=d_ff, horizon=args.horizon, input_len=args.input_len)\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu')\n",
        "        model = model.to(device)\n",
        "        train_ds = TimeSeriesWindowDataset(train_scaled, args.input_len, args.horizon)\n",
        "        val_ds = TimeSeriesWindowDataset(val_scaled, args.input_len, args.horizon)\n",
        "        train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, collate_fn=collate_windows)\n",
        "        val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, collate_fn=collate_windows)\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        loss_fn = nn.MSELoss()\n",
        "        best_val = float('inf')\n",
        "        for epoch in range(1, args.epochs+1):\n",
        "            train_one_epoch(model, train_loader, opt, loss_fn, device)\n",
        "            val_loss = validate(model, val_loader, loss_fn, device)\n",
        "            trial.report(val_loss, epoch)\n",
        "            if trial.should_prune():\n",
        "                raise optuna.exceptions.TrialPruned()\n",
        "            best_val = min(best_val, val_loss)\n",
        "        return best_val\n",
        "\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=args.trials)\n",
        "    print(\"Best params:\", study.best_trial.params)\n",
        "    os.makedirs(EXPERIMENTS_DIR, exist_ok=True)\n",
        "    pd.DataFrame([study.best_trial.params]).to_csv(os.path.join(EXPERIMENTS_DIR, 'best_params.csv'), index=False)\n",
        "\n",
        "# ---------------------------\n",
        "# Helpers: write requirements, run_all.sh, tests, multi-seed script\n",
        "# ---------------------------\n",
        "def write_requirements():\n",
        "    txt = \"\"\"numpy\n",
        "pandas\n",
        "matplotlib\n",
        "scikit-learn\n",
        "torch\n",
        "optuna\n",
        "prophet\n",
        "\"\"\"\n",
        "    with open(\"requirements.txt\", \"w\") as f:\n",
        "        f.write(txt)\n",
        "    print(\"Wrote requirements.txt\")\n",
        "\n",
        "def write_run_all():\n",
        "    content = \"\"\"#!/usr/bin/env bash\n",
        "set -e\n",
        "python3 -m venv venv\n",
        "source venv/bin/activate\n",
        "pip install -r requirements.txt\n",
        "# generate synthetic dataset\n",
        "python3 advanced_ts_transformer_complete.py --generate --length 15000\n",
        "# quick train (reduced epochs for quick run)\n",
        "python3 advanced_ts_transformer_complete.py --generate --length 15000 --epochs 4 --input-len 256 --horizon 50\n",
        "# run a short hp-search (optional, takes time)\n",
        "python3 advanced_ts_transformer_complete.py --hp-search --trials 3 --epochs 5\n",
        "# run multi-seed eval\n",
        "python3 advanced_ts_transformer_complete.py --multi-seed --seeds 3\n",
        "ls -la results\n",
        "\"\"\"\n",
        "    with open(\"run_all.sh\", \"w\") as f:\n",
        "        f.write(content)\n",
        "    os.chmod(\"run_all.sh\", 0o755)\n",
        "    print(\"Wrote run_all.sh\")\n",
        "\n",
        "def write_tests_and_scripts():\n",
        "    # tests/test_model_shapes.py\n",
        "    test_content = '''import torch\n",
        "from advanced_ts_transformer_complete import TransformerForecaster, LSTMForecaster\n",
        "\n",
        "def test_transformer_shapes():\n",
        "    B, C, L = 2, 1, 128\n",
        "    x = torch.randn(B, C, L)\n",
        "    m = TransformerForecaster(input_dim=1, d_model=32, n_heads=4, n_layers=2, d_ff=64, horizon=10, input_len=L)\n",
        "    y, attn = m(x, return_attn=True)\n",
        "    assert y.shape == (B, 1, 10)\n",
        "\n",
        "def test_lstm_shapes():\n",
        "    B, C, L = 2, 1, 128\n",
        "    x = torch.randn(B, C, L)\n",
        "    m = LSTMForecaster(input_dim=1, hidden=32, n_layers=1, horizon=10)\n",
        "    y = m(x)\n",
        "    assert y.shape == (B, 1, 10)\n",
        "'''\n",
        "    os.makedirs(TESTS_DIR, exist_ok=True)\n",
        "    with open(os.path.join(TESTS_DIR, \"test_model_shapes.py\"), \"w\") as f:\n",
        "        f.write(test_content)\n",
        "\n",
        "    # multi_seed_eval.py (short wrapper)\n",
        "    multi_content = '''import argparse, subprocess, pandas as pd\n",
        "def run_seed(seed):\n",
        "    cmd = f\"python3 advanced_ts_transformer_complete.py --generate --length 15000 --epochs 4 --seed {seed}\"\n",
        "    print(\"Running:\", cmd)\n",
        "    subprocess.run(cmd, shell=True, check=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--seeds\", type=int, default=3)\n",
        "    args = p.parse_args()\n",
        "    metrics = []\n",
        "    for s in range(args.seeds):\n",
        "        run_seed(42 + s)\n",
        "        df = pd.read_csv(\"results/metrics_transformer.csv\")\n",
        "        metrics.append(df)\n",
        "    combined = pd.concat(metrics)\n",
        "    agg = combined.groupby([\"horizon_eval\",\"step\"]).agg({\"rmse\":[\"mean\",\"std\"], \"mae\":[\"mean\",\"std\"], \"mape\":[\"mean\",\"std\"]})\n",
        "    print(agg)\n",
        "    agg.to_csv(\"results/aggregate_metrics.csv\")\n",
        "'''\n",
        "    with open(\"multi_seed_eval.py\", \"w\") as f:\n",
        "        f.write(multi_content)\n",
        "\n",
        "    print(\"Wrote tests and multi_seed_eval.py\")\n",
        "\n",
        "# ---------------------------\n",
        "# CLI\n",
        "# ---------------------------\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser(description=\"Complete single-file project for Transformer time series forecasting\")\n",
        "    p.add_argument('--generate', action='store_true', help='Generate synthetic dataset')\n",
        "    p.add_argument('--length', type=int, default=15000, help='Length of synthetic series to generate')\n",
        "    p.add_argument('--freq', type=str, default='H', help='Frequency for datetime index')\n",
        "    p.add_argument('--data-path', type=str, default=os.path.join(DATA_DIR, 'synthetic_15000.csv'))\n",
        "    p.add_argument('--model', type=str, default='transformer', choices=['transformer','lstm'])\n",
        "    p.add_argument('--input-len', type=int, dest='input_len', default=256)\n",
        "    p.add_argument('--horizon', type=int, default=50)\n",
        "    p.add_argument('--batch-size', type=int, dest='batch_size', default=64)\n",
        "    p.add_argument('--d-model', type=int, dest='d_model', default=128)\n",
        "    p.add_argument('--n-heads', type=int, dest='n_heads', default=4)\n",
        "    p.add_argument('--n-layers', type=int, dest='n_layers', default=3)\n",
        "    p.add_argument('--d-ff', type=int, dest='d_ff', default=256)\n",
        "    p.add_argument('--dropout', type=float, default=0.1)\n",
        "    p.add_argument('--hidden', type=int, default=128)\n",
        "    p.add_argument('--lr', type=float, default=1e-4)\n",
        "    p.add_argument('--epochs', type=int, default=30)\n",
        "    p.add_argument('--patience', type=int, default=6)\n",
        "    p.add_argument('--seed', type=int, default=42)\n",
        "    p.add_argument('--no-cuda', action='store_true')\n",
        "    # special actions\n",
        "    p.add_argument('--hp-search', action='store_true', help='Run integrated Optuna HP search')\n",
        "    p.add_argument('--trials', type=int, default=20, help='HP search trials')\n",
        "    p.add_argument('--multi-seed', action='store_true', help='Run multi-seed evaluation wrapper (reads/writes results files)')\n",
        "    p.add_argument('--seeds', type=int, default=3)\n",
        "    p.add_argument('--write-helpers', action='store_true', help='Write requirements, run_all.sh, tests and multi-seed script')\n",
        "    return p.parse_args()\n",
        "\n",
        "# ---------------------------\n",
        "# Main\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "\n",
        "    if args.write_helpers:\n",
        "        write_requirements()\n",
        "        write_run_all()\n",
        "        write_tests_and_scripts()\n",
        "        print(\"Helpers written. Run ./run_all.sh to install and run a quick pipeline.\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    if args.hp_search:\n",
        "        run_hp_search(args)\n",
        "        sys.exit(0)\n",
        "\n",
        "    if args.multi_seed:\n",
        "        # call the generated script if exists; otherwise run simple repeats\n",
        "        if os.path.exists(\"multi_seed_eval.py\"):\n",
        "            subprocess.run([\"python3\", \"multi_seed_eval.py\", \"--seeds\", str(args.seeds)], check=True)\n",
        "        else:\n",
        "            # fallback simple multi-run\n",
        "            all_metrics = []\n",
        "            for s in range(args.seeds):\n",
        "                args.seed = 42 + s\n",
        "                run_experiment(args)\n",
        "                df = pd.read_csv(os.path.join(OUT_DIR, f\"metrics_{args.model}.csv\"))\n",
        "                all_metrics.append(df)\n",
        "            combined = pd.concat(all_metrics)\n",
        "            agg = combined.groupby([\"horizon_eval\",\"step\"]).agg({\"rmse\":[\"mean\",\"std\"], \"mae\":[\"mean\",\"std\"], \"mape\":[\"mean\",\"std\"]})\n",
        "            agg.to_csv(os.path.join(OUT_DIR, \"aggregate_metrics.csv\"))\n",
        "            print(\"Wrote aggregate_metrics.csv\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    # default: run experiment\n",
        "    run_experiment(args)\n"
      ]
    }
  ]
}